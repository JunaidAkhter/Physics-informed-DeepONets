{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyND4f99CmzfUOPve1DqrbEc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JunaidAkhter/Physics-informed-DeepONets/blob/main/PytorchAntider.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ulYqHZCP6Hxz"
      },
      "outputs": [],
      "source": [
        "import numpy as onp\n",
        "import jax.numpy as np\n",
        "from jax import random, grad, vmap, jit\n",
        "from jax.example_libraries import optimizers\n",
        "from jax.experimental.ode import odeint\n",
        "from jax.nn import relu\n",
        "from jax.config import config\n",
        "\n",
        "import itertools\n",
        "from functools import partial\n",
        "from torch.utils import data\n",
        "from tqdm import trange\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Let us generate the data first. \n",
        "We use RBF to generate the training and the testing data. \\\\\n",
        "**Note:** The data is being generated using `Jax`. However, we use Pytorch for learning. Hence we convert the generated data to numpy arrays which is later on converted to `torch` tensors. "
      ],
      "metadata": {
        "id": "fUKGNYq9CXec"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title RBF and data generation. \n",
        "# Length scale of a Gaussian random field (GRF)\n",
        "length_scale = 0.2\n",
        "\n",
        "# Define RBF kernel\n",
        "def RBF(x1, x2, params):\n",
        "    output_scale, lengthscales = params\n",
        "    diffs = np.expand_dims(x1 / lengthscales, 1) - \\\n",
        "            np.expand_dims(x2 / lengthscales, 0)\n",
        "    r2 = np.sum(diffs**2, axis=2)\n",
        "    return output_scale * np.exp(-0.5 * r2)\n",
        "\n",
        "# Geneate training data corresponding to one input sample\n",
        "def generate_one_training_data(key, m=100, P=1):\n",
        "    # Sample GP prior at a fine grid\n",
        "    N = 512\n",
        "    gp_params = (1.0, length_scale)\n",
        "    jitter = 1e-10\n",
        "    X = np.linspace(0, 1, N)[:,None]\n",
        "    K = RBF(X, X, gp_params)\n",
        "    L = np.linalg.cholesky(K + jitter*np.eye(N))\n",
        "    gp_sample = np.dot(L, random.normal(key, (N,)))\n",
        "\n",
        "    # Create a callable interpolation function  \n",
        "    u_fn = lambda x, t: np.interp(t, X.flatten(), gp_sample)\n",
        "\n",
        "    # Input sensor locations and measurements\n",
        "    x = np.linspace(0, 1, m)\n",
        "    u = vmap(u_fn, in_axes=(None,0))(0.0, x)\n",
        "\n",
        "    # Output sensor locations and measurements\n",
        "    y_train = random.uniform(key, (P,)).sort() \n",
        "    s_train = odeint(u_fn, 0.0, np.hstack((0.0, y_train)))[1:] # JAX has a bug and always returns s(0), so add a dummy entry to y and return s[1:]\n",
        "\n",
        "    # Tile inputs\n",
        "    u_train = np.tile(u, (P,1))\n",
        "\n",
        "    # training data for the residual\n",
        "    u_r_train = np.tile(u, (m, 1))  # CREATES m COPIES of u.  \n",
        "    y_r_train = x\n",
        "    s_r_train = u    # STUPID NAMING WALLAHI\n",
        "\n",
        "    #print(\"shape of u_r_train:\", u_r_train.shape)\n",
        "    #print(\"shape of s_r_train:\", s_r_train.shape)\n",
        "\n",
        "    return u_train, y_train, s_train, u_r_train, y_r_train,  s_r_train\n",
        "\n",
        "# Geneate test data corresponding to one input sample\n",
        "def generate_one_test_data(key, m=100, P=100):\n",
        "    # Sample GP prior at a fine grid\n",
        "    N = 512\n",
        "    gp_params = (1.0, length_scale)\n",
        "    jitter = 1e-10\n",
        "    X = np.linspace(0, 1, N)[:,None]\n",
        "    K = RBF(X, X, gp_params)\n",
        "    L = np.linalg.cholesky(K + jitter*np.eye(N))\n",
        "    gp_sample = np.dot(L, random.normal(key, (N,)))\n",
        "\n",
        "    # Create a callable interpolation function  \n",
        "    u_fn = lambda x, t: np.interp(t, X.flatten(), gp_sample)\n",
        "\n",
        "    # Input sensor locations and measurements\n",
        "    x = np.linspace(0, 1, m)\n",
        "    u = vmap(u_fn, in_axes=(None,0))(0.0, x)\n",
        "\n",
        "    # Output sensor locations and measurements\n",
        "    y = np.linspace(0, 1, P)\n",
        "    s = odeint(u_fn, 0.0, y)\n",
        "\n",
        "    # Tile inputs\n",
        "    u = np.tile(u, (P,1))\n",
        "\n",
        "    return u, y, s \n",
        "\n",
        "# Geneate training data corresponding to N input sample\n",
        "def generate_training_data(key, N, m, P):\n",
        "    config.update(\"jax_enable_x64\", True)\n",
        "    keys = random.split(key, N)\n",
        "    gen_fn = jit(lambda key: generate_one_training_data(key, m, P))\n",
        "    u_train, y_train, s_train, u_r_train, y_r_train, s_r_train = vmap(gen_fn)(keys)\n",
        "\n",
        "    u_train = np.float32(u_train.reshape(N * P,-1))\n",
        "    y_train = np.float32(y_train.reshape(N * P,-1))\n",
        "    s_train = np.float32(s_train.reshape(N * P,-1))\n",
        "\n",
        "    u_r_train = np.float32(u_r_train.reshape(N * m,-1))\n",
        "    y_r_train = np.float32(y_r_train.reshape(N * m,-1))\n",
        "    s_r_train = np.float32(s_r_train.reshape(N * m,-1))\n",
        "\n",
        "    config.update(\"jax_enable_x64\", False)\n",
        "    return u_train, y_train, s_train, u_r_train, y_r_train,  s_r_train\n",
        "\n",
        "# Geneate test data corresponding to N input sample\n",
        "def generate_test_data(key, N, m, P):\n",
        "    config.update(\"jax_enable_x64\", True)\n",
        "    keys = random.split(key, N)\n",
        "    gen_fn = jit(lambda key: generate_one_test_data(key, m, P))\n",
        "    u, y, s = vmap(gen_fn)(keys)\n",
        "    u = np.float32(u.reshape(N * P,-1))\n",
        "    y = np.float32(y.reshape(N * P,-1))\n",
        "    s = np.float32(s.reshape(N * P,-1))\n",
        "\n",
        "    config.update(\"jax_enable_x64\", False)\n",
        "    return u, y, s"
      ],
      "metadata": {
        "id": "nUUN_m8l6QsS"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title creating training data and converting jax.numpy to onp\n",
        "N_train = 10000 # number of input samples\n",
        "m = 100 # number of input sensors\n",
        "P_train = 1   # number of output sensors\n",
        "key_train = random.PRNGKey(0) # use different key for generating training data and test data \n",
        "u_train, y_train, s_train, u_r_train, y_r_train, s_r_train = generate_training_data(key_train, N_train, m, P_train)\n",
        "\n",
        "#changing to numpy \n",
        "u_train, y_train, s_train, u_r_train, y_r_train, s_r_train = onp.array(u_train), onp.array(y_train), onp.array(s_train), onp.array(u_r_train), onp.array(y_r_train), onp.array(s_r_train) \n",
        "\n",
        "print(\"type of data that we have now:\", type(u_train), type(y_train), type(s_train), type(u_r_train), type(y_r_train), type(s_r_train))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E4T4CD8O6ZQl",
        "outputId": "2344125a-5f7f-4a63-b2d7-a35294ad5488"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:jax._src.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "type of data that we have now: <class 'numpy.ndarray'> <class 'numpy.ndarray'> <class 'numpy.ndarray'> <class 'numpy.ndarray'> <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate test data\n",
        "N_test = 100\n",
        "P_test = m\n",
        "key_test = random.PRNGKey(12345)\n",
        "keys_test = random.split(key_test, N_test)\n",
        "\n",
        "u_test, y_test, s_test =  generate_test_data(key_test, N_test, m, m)\n",
        "\n",
        "u_test, y_test, s_test = onp.array(u_test), onp.array(y_test), onp.array(s_test)\n",
        "print(type(u_test), type(y_test), type(s_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nI5ztq7hB7J7",
        "outputId": "1bdf6de6-a6e1-4a44-f510-d7226b0d14a1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'numpy.ndarray'> <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# THIS WHOLE SCRIPT CAN BE WRITTEN WITH THE TEMPLATE OF OUR PACKAGE AND IT IS GOING TO LOOK BEAUTIFUL.\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.autograd import grad\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim.lr_scheduler import ExponentialLR\n",
        "import itertools\n",
        "from functools import partial\n",
        "from torch.utils import data\n",
        "from tqdm import trange\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset\n",
        "from typing import Callable\n"
      ],
      "metadata": {
        "id": "WJOQZ7O66gTM"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DataGenerator(Dataset):\n",
        "\n",
        "    \"\"\"The inputs should be provided as numpy arrays\"\"\"\n",
        "\n",
        "    def __init__(self, u, y, s, batch_size=64):\n",
        "        self.u = torch.tensor(u, requires_grad=True)  # Convert to PyTorch tensor\n",
        "        self.y = torch.tensor(y, requires_grad=True)  # Convert to PyTorch tensor\n",
        "        self.s = torch.tensor(s, requires_grad=True)  # Convert to PyTorch tensor\n",
        "        \n",
        "        self.N = u.shape[0]\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.N // self.batch_size\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        start = index * self.batch_size\n",
        "        end = (index + 1) * self.batch_size\n",
        "\n",
        "        inputs, outputs = self.__data_generation(start, end)\n",
        "        return inputs, outputs\n",
        "\n",
        "    def __data_generation(self, start, end):\n",
        "        idx = torch.randperm(self.N, device=self.u.device)[start:end]\n",
        "        s = self.s[idx]\n",
        "        y = self.y[idx]\n",
        "        u = self.u[idx]\n",
        "        \n",
        "        inputs = (u, y)\n",
        "        outputs = s\n",
        "        return inputs, outputs\n"
      ],
      "metadata": {
        "id": "K-ksv31J70Y2"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create data set\n",
        "batch_size = 10000\n",
        "operator_dataset = DataGenerator(u_train, y_train, s_train, batch_size)\n",
        "physics_dataset = DataGenerator(u_r_train, y_r_train, s_r_train, batch_size)\n",
        "operator_data = DataLoader(operator_dataset, batch_size=64, shuffle=True)\n",
        "physics_data = DataLoader(physics_dataset, batch_size=64, shuffle=True)\n"
      ],
      "metadata": {
        "id": "IZXSqXTg78DF"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Vanilla PyTorch model\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, layers, activation=F.relu):\n",
        "        super(MLP, self).__init__()\n",
        "        self.activation = activation\n",
        "        self.layers = nn.ModuleList()\n",
        "        for i in range(len(layers)-1):\n",
        "            self.layers.append(nn.Linear(layers[i], layers[i+1]))\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.layers[:-1]:\n",
        "            x = self.activation(layer(x))\n",
        "        x = self.layers[-1](x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "tEC_UDe38JIm"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class DeepONet(nn.Module):\n",
        "    def __init__(self, branch_layers, trunk_layers):    \n",
        "        super(DeepONet, self).__init__()\n",
        "\n",
        "        #TODO: Make sure that the parameters are getting updated by writing the model this way\n",
        "\n",
        "        \n",
        "        # Network initialization and evaluation functions\n",
        "        self.branch = MLP(branch_layers, torch.tanh)\n",
        "        self.trunk = MLP(trunk_layers, torch.tanh)\n",
        "\n",
        "        # TODO: Make sure that model parameters are being fed to the optimizer. Do we need to feed the parameters from both models?\n",
        "        # TODO: Chat GPT tells that we need concatenate the model parameters from branch and trunk networks and feed it to the optimizer like below.  \n",
        "\n",
        "        #parameters = list(model1.parameters()) + list(model2.parameters())\n",
        "        # Define the optimizer\n",
        "        \n",
        "        # Define DeepONet architecture\n",
        "    def forward(self, u, y):\n",
        "        B = self.branch(u)\n",
        "        T = self.trunk(y)\n",
        "        #print(\"B\", B)\n",
        "        outputs = torch.sum(B * T, dim=-1)                                   #WHY IS dim = -1 here?\n",
        "        return outputs\n"
      ],
      "metadata": {
        "id": "MPZka2GC8QBL"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def s(model: nn.Module(), u: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"Compute the value of the approximate solution from the DeepONet model\"\"\"\n",
        "    return model(u, y)\n",
        "\n",
        "\n",
        "def ds(model: nn.Module(), u: torch.Tensor, y: torch.Tensor, order: int = 1) -> torch.Tensor:\n",
        "    \"\"\"Compute neural network derivative with respect to input features using PyTorch autograd engine\"\"\"\n",
        "\n",
        "    df_value = s(model, u, y)\n",
        "\n",
        "\n",
        "    for _ in range(order):\n",
        "        df_value = torch.autograd.grad(\n",
        "            df_value.reshape((-1, 1)),\n",
        "            y,\n",
        "            grad_outputs=torch.ones_like(y),\n",
        "            create_graph=True,\n",
        "            retain_graph=True,\n",
        "        )[0]\n",
        "\n",
        "    return df_value\n"
      ],
      "metadata": {
        "id": "tNssBTyi8V0_"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def residue(model:nn.Module(), u, y):\n",
        "\n",
        "    #TODO: adapt it to higer order derrivatives. Maybe we need to create a resideue class and define df inside it. \n",
        "\n",
        "    return ds(model,u, y)\n",
        "\n",
        "\n",
        "# Define operator loss\n",
        "def loss_operator(model:nn.Module, batch):\n",
        "    inputs, outputs = batch\n",
        "    u, y = inputs\n",
        "    \n",
        "    pred = s(model, u, y)\n",
        "\n",
        "    #pred = self(u, y)                                                  #STUPID LINE BY CHAT GPT\n",
        "    loss = torch.mean((outputs.view(-1) - pred.view(-1))**2)\n",
        "    return loss\n",
        "\n",
        "def loss_physics(model:nn.Module(), batch):\n",
        "\n",
        "    #TODO: clear the air regarding this loss. I think the formulation in the original code is wrong. \n",
        "\n",
        "    inputs, outputs = batch\n",
        "    u, y = inputs\n",
        "    \n",
        "    pred = residue(model, u, y)\n",
        "    \n",
        "    loss = torch.mean((outputs.view(-1) - pred.view(-1))**2)\n",
        "\n",
        "    return loss \n",
        "\n",
        "\n",
        "def total_loss(model:nn.Module(), operator_batch, physics_batch):\n",
        "    \"\"\"Summing up the two losses\"\"\"\n",
        "    #TODO: One can think of weighed sum of the two losses instead of plain sum. \n",
        "    loss_op = loss_operator(model, operator_batch)\n",
        "    loss_ph = loss_physics(model, physics_batch)\n",
        "\n",
        "    return loss_op + loss_ph\n"
      ],
      "metadata": {
        "id": "gBGAN-mt8Z5Y"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def train(model:nn.Module(), \n",
        "    operator_dataset, \n",
        "    physics_dataset, \n",
        "#    loss_fn: Callable,  #TODO: Make this callable like PINN script\n",
        "    learning_rate: \n",
        "    int = 0.01,\n",
        "    max_epochs: int = 4_0000,\n",
        ")-> nn.Module():\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    operator_data = iter(operator_dataset)\n",
        "    physics_data = iter(physics_dataset)\n",
        "\n",
        "    pbar = trange(max_epochs)\n",
        "    for epoch in pbar:\n",
        "        operator_batch = next(operator_data)\n",
        "        physics_batch = next(physics_data)\n",
        "\n",
        "\n",
        "            \n",
        "        #Optimization step\n",
        "        loss: torch.Tensor = total_loss(model, operator_batch, physics_batch)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if epoch % 50 == 0:\n",
        "            print(f\"Epoch: {epoch} - Loss: {float(loss):>7f}\")\n",
        "\n",
        "\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "-xhjdzTr8fXD"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the object PI_DeepOneta\n",
        "m = 100\n",
        "branch_layers = [m, 50, 50, 50, 50, 50]\n",
        "trunk_layers =  [1, 50, 50, 50, 50, 50]\n",
        "model = DeepONet(branch_layers, trunk_layers)"
      ],
      "metadata": {
        "id": "bdjWUZ9X8jB1"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#LET US TRAIN THE NETWORK NOW\n",
        "#loss_fn = partial(total_loss, ) #TODO: complete this to make loss_fn callable\n",
        "trained_model = train(model, operator_dataset, physics_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 662
        },
        "id": "yejH6kkg8oAW",
        "outputId": "2255faf6-ecd4-4866-d810-e7e12e281d4a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 1/40000 [00:00<6:20:27,  1.75it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 - Loss: 1.206528\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 51/40000 [00:26<4:52:53,  2.27it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 50 - Loss:     nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 101/40000 [00:52<4:59:41,  2.22it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 100 - Loss:     nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 151/40000 [01:13<4:59:20,  2.22it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 150 - Loss:     nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  1%|          | 201/40000 [01:36<5:28:33,  2.02it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 200 - Loss:     nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  1%|          | 251/40000 [01:59<5:36:52,  1.97it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 250 - Loss:     nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  1%|          | 301/40000 [02:18<3:49:15,  2.89it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 300 - Loss:     nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  1%|          | 351/40000 [02:39<3:50:52,  2.86it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 350 - Loss:     nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  1%|          | 401/40000 [02:59<4:38:54,  2.37it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 400 - Loss:     nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  1%|          | 451/40000 [03:20<5:02:41,  2.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 450 - Loss:     nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  1%|▏         | 501/40000 [03:42<4:07:47,  2.66it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 500 - Loss:     nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  1%|▏         | 551/40000 [04:03<5:49:35,  1.88it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 550 - Loss:     nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  2%|▏         | 601/40000 [04:24<4:21:41,  2.51it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 600 - Loss:     nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  2%|▏         | 651/40000 [04:45<4:00:57,  2.72it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 650 - Loss:     nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  2%|▏         | 701/40000 [05:06<4:35:02,  2.38it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 700 - Loss:     nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  2%|▏         | 744/40000 [05:24<4:45:36,  2.29it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-c1bbc482173a>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#LET US TRAIN THE NETWORK NOW\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#loss_fn = partial(total_loss, ) #TODO: complete this to make loss_fn callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrained_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperator_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphysics_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-15-f13ea119ac5a>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, operator_dataset, physics_dataset, learning_rate, max_epochs)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperator_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphysics_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "H3AH6h1sBA3R"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vqKVtI2VBEfv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}